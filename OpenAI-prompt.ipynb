{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The aim of this project is to use OpenAI's API to generate text and test the LLM. This is a personal project just to practise the use of OpenAI's API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libaries\n",
    "\n",
    "import openai\n",
    "import my_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning my api key\n",
    "api_key = my_key.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning my api key to open ai\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt):\n",
    "    response = openai.Completion.create(\n",
    "        model= \"gpt-3.5-turbo-instruct\", #text-davinci-002, Correct parameter name and value since davinci no longer available\n",
    "        prompt=prompt,\n",
    "        max_tokens=10,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# can also use response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Once upon a time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text =  generate_text(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in a land far far away\n",
      "\n",
      "There lived a\n"
     ]
    }
   ],
   "source": [
    "print(prompt, generate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing the Output\n",
    "\n",
    "#### Max Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text1(prompt, max_tokens):\n",
    "    response = openai.Completion.create(\n",
    "        model= \"gpt-3.5-turbo-instruct\", #text-davinci-002, Correct parameter name and value since davinci no longer available\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time , a princess was born\n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text1(prompt, 5)\n",
    "print(prompt, generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time , there lived three sisters. The oldest sister was named Lily and she was known for her beauty and\n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text1(prompt, 20)\n",
    "print(prompt, generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature\n",
    "The more temperature the more randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text2(prompt, max_tokens, temperature):\n",
    "    response = openai.Completion.create(\n",
    "        model= \"gpt-3.5-turbo-instruct\", #text-davinci-002, Correct parameter name and value since davinci no longer available\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time , there was a little girl named Goldilocks. She lived in a small cottage in the woods with her parents. One day, while out for a walk, Goldilocks stumbled upon a beautiful house in the middle of the forest. Curiosity\n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text2(prompt, 50, 0) # max token 50 to see the difference, temp 0\n",
    "print(prompt, generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time ... in a galaxy far, far away... there was a boy named Luke Skywalker.</p>\n",
      "<p>Luke lived on a small, remote planet called Tatooine with his aunt and uncle. They were moisture farmers, scraping by a living on the\n"
     ]
    }
   ],
   "source": [
    "# temeprature of 1\n",
    "generated_text = generate_text2(prompt, 50, 1) # max token 50 to see the difference, temp 1\n",
    "print(prompt, generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summerizing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_summarizer(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "        {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": \"You will be provided with a block of text, and your task is to extract a list of keywords from it.\"\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"A flying saucer seen by a guest house, a 7ft alien-like figure coming out of a hedge and a \\\"cigar-shaped\\\" UFO near a school yard.\\n\\nThese are just some of the 450 reported extraterrestrial encounters from one of the UK's largest mass sightings in a remote Welsh village.\\n\\nThe village of Broad Haven has since been described as the \\\"Bermuda Triangle\\\" of mysterious craft sightings and sightings of strange beings.\\n\\nResidents who reported these encounters across a single year in the late seventies have now told their story to the new Netflix documentary series 'Encounters', made by Steven Spielberg's production company.\\n\\nIt all happened back in 1977, when the Cold War was at its height and Star Wars and Close Encounters of the Third Kind - Spielberg's first science fiction blockbuster - dominated the box office.\"\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"assistant\",\n",
    "          \"content\": \"flying saucer, guest house, 7ft alien-like figure, hedge, cigar-shaped UFO, school yard, extraterrestrial encounters, UK, mass sightings, remote Welsh village, Broad Haven, Bermuda Triangle, mysterious craft sightings, strange beings, residents, single year, late seventies, Netflix documentary series, Steven Spielberg, production company, 1977, Cold War, Star Wars, Close Encounters of the Third Kind, science fiction blockbuster, box office.\"\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"Each April, in the village of Maeliya in northwest Sri Lanka, Pinchal Weldurelage Siriwardene gathers his community under the shade of a large banyan tree. The tree overlooks a human-made body of water called a wewa â€“ meaning reservoir or \\\"tank\\\" in Sinhala. The wewa stretches out besides the village's rice paddies for 175-acres (708,200 sq m) and is filled with the rainwater of preceding months.    \\n\\nSiriwardene, the 76-year-old secretary of the village's agrarian committee, has a tightly-guarded ritual to perform. By boiling coconut milk on an open hearth beside the tank, he will seek blessings for a prosperous harvest from the deities residing in the tree. \\\"It's only after that we open the sluice gate to water the rice fields,\\\" he told me when I visited on a scorching mid-April afternoon.\\n\\nBy releasing water into irrigation canals below, the tank supports the rice crop during the dry months before the rains arrive. For nearly two millennia, lake-like water bodies such as this have helped generations of farmers cultivate their fields. An old Sinhala phrase, \\\"wewai dagabai gamai pansalai\\\", even reflects the technology's centrality to village life; meaning \\\"tank, pagoda, village and temple\\\".\"\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"assistant\",\n",
    "          \"content\": \"April, Maeliya, northwest Sri Lanka, Pinchal Weldurelage Siriwardene, banyan tree, wewa, reservoir, tank, Sinhala, rice paddies, 175-acres, 708,200 sq m, rainwater, agrarian committee, coconut milk, open hearth, blessings, prosperous harvest, deities, sluice gate, rice fields, irrigation canals, dry months, rains, lake-like water bodies, farmers, cultivate, Sinhala phrase, technology, village life, pagoda, temple.\"\n",
    "        }, \n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": prompt\n",
    "        }\n",
    "      ],\n",
    "      temperature=0.5,\n",
    "      max_tokens=256\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jesus is described as the Word of God, embodying divine wisdom and truth. According to the Bible, in the beginning was the Word, and the Word was with God, and the Word was God. This signifies Jesus' eternal existence, his role in creation, and his divine nature as the ultimate revelation of God's will and character.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Jesus is described as the Word of God, embodying divine wisdom and truth. According to the Bible, in the beginning was the Word, and the Word was with God, and the Word was God. This signifies Jesus' eternal existence, his role in creation, and his divine nature as the ultimate revelation of God's will and character.\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jesus, Word of God, divine wisdom, truth, Bible, beginning, God, eternal existence, creation, divine nature, revelation, will, character.\n"
     ]
    }
   ],
   "source": [
    "text = text_summarizer(prompt)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above exampl e we see where unlike hugging face where we had to import a model for text summerization, we can just use prompts, chat GPT's API allows us to use the same model to do multiple tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Poetic Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poetic_chatbot(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model = \"gpt-3.5-turbo\",\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a poetic chatbot.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"When was Google founded?\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"In the late '90s, a spark did ignite, Google emerged, a radiant light. By Larry and Sergey, in '98, it was born, a search engine new, on the web it was sworn.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Which country has the youngest president?\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Ah, the pursuit of youth in politics, a theme we explore. In Austria, Sebastian Kurz did implore, at the age of 31, his journey did begin, leading with vigor, in a world filled with din.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature = 1,\n",
    "        max_tokens=256\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the Aegean Sea, a gem does lie, Patmos is its name, under the Grecian sky. A place of beauty, of history untold, where ancient tales and mysteries unfold.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Where is isle of Patmos\"\n",
    "poem = poetic_chatbot(prompt)\n",
    "print(poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the realm of computing power, a marvel to behold, Google's Quantum chip, a tale so bold. Using 53 qubits, its speed did shine, solving a task in just five minutes time. For today's supercomputers to match this feat, the time they'd take would be no small feat. It is estimated that it would take a supercomputer thousands of years to compute the same problem that Google's Quantum chip did in 5 minutes, using 53 qubits.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"How long wull today's super computers take to compute the same problem that Google's Quantum chip did in 5 minutes? Give the exact time it would take super computers and how many qubits used in the quantum chip.\"\n",
    "poem2 = poetic_chatbot(prompt)\n",
    "print(poem2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this output I can tell that the 2024 information published by Google that was recently published using 105 qubits was not apart of the training data as it is new, and therefore the model is unable to answer correctly. This is where updating the mnodel with recent information comes in.\n",
    "\n",
    "53 qubits was used on the Sycamore Processor in 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "LangChain is a framework that helps developers build applications using large language models (LLMs) by enabling integration with external data sources and managing workflows.\n",
    "Since every model has a cutoff date of the data used to train it, Langchain in a sense helps to update the model with up to date information.\n",
    "\n",
    "Use Cases:\n",
    "- Chatbots and conversational agents.\n",
    "- Document summarization and question answering.\n",
    "- Data querying and analysis (e.g., using SQL).\n",
    "- Workflow automation.\n",
    "- Context-aware AI tools for specific industries (e.g., legal, healthcare).\n",
    "- LangChain streamlines the integration of LLMs into sophisticated applications, making it easier for developers to harness the full potential of AI models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://blog.google/technology/research/google-willow-quantum-chip/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use different types of loaders but because we want to use a url we will use webbase loader\n",
    "loader = WebBaseLoader(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract raw document\n",
    "\n",
    "raw_documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take data and chunk it up into small pieces to make it more digestable for the model. Then use those to create document\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tevin\\AppData\\Local\\Temp\\ipykernel_17948\\3637299338.py:2: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n"
     ]
    }
   ],
   "source": [
    "# create embeddings usin OPENAI Embeddings\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After creating embedding we use vector store on the documents and embeddings\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tevin\\AppData\\Local\\Temp\\ipykernel_17948\\3812877406.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key = \"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "# create a memory object to store inout and output of the model to hold a conversation\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key = \"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tevin\\AppData\\Local\\Temp\\ipykernel_17948\\3752287279.py:3: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  qa = ConversationalRetrievalChain.from_llm(OpenAI(openai_api_key=api_key, temperature=0), vectorstore.as_retriever(), memory=memory)\n"
     ]
    }
   ],
   "source": [
    "# initialize conversation retrieval chain, combine chat history and chat, look to the retriever to a question answering chain to answer correctly\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(OpenAI(openai_api_key=api_key, temperature=0), vectorstore.as_retriever(), memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How long wull today's super computers take to compute the same problem that Google's Quantum chip did in 5 minutes? Give the exact time it would take super computers and how many qubits used in the quantum chip.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tevin\\AppData\\Local\\Temp\\ipykernel_17948\\366029401.py:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa({\"question\": query})\n"
     ]
    }
   ],
   "source": [
    "result = qa({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Today's supercomputers would take 10 septillion years to compute the same problem that Google's Quantum chip did in 5 minutes. The quantum chip used 105 qubits.\n"
     ]
    }
   ],
   "source": [
    "answer = result[\"answer\"]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can now see that the model can answer correctly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
